{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentence_creation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbQG39Ukkl9N"
      },
      "source": [
        "# 자연어 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYR3xg5jknq3"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00OTiTkxk2m9",
        "outputId": "364bf78b-70e3-4297-d70f-84941c724907"
      },
      "source": [
        "# 데이터 블러오기\n",
        "# Naver movie corpus 불러오기\n",
        "file = tf.keras.utils.get_file('ratings_train.txt',\n",
        "  origin='https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt',\n",
        "  extract=True)\n",
        "df = pd.read_csv(file, sep='\\t')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
            "14630912/14628807 [==============================] - 0s 0us/step\n",
            "14639104/14628807 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "xuR2y5allLPc",
        "outputId": "03645981-24fa-4cb6-f2f9-a64886ae6fbd"
      },
      "source": [
        "df[1000:1007]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1000</th>\n",
              "      <td>9856453</td>\n",
              "      <td>정말 최고의 명작 성인이 되고 본 이집트의 왕자는 또 다른 감동 그자체네요</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1001</th>\n",
              "      <td>6961803</td>\n",
              "      <td>이영화만 성공 했어도 스퀘어가 에닉스랑 합병 할일은 없었을텐데..</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1002</th>\n",
              "      <td>8681713</td>\n",
              "      <td>울컥하는 사회현실 ㅠㅠ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1003</th>\n",
              "      <td>5348290</td>\n",
              "      <td>기대를하나도안하면 할일없을때보기좋은영화</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1004</th>\n",
              "      <td>9340549</td>\n",
              "      <td>소림사 관문 통과하기 진짜 어렵다는거 보여준 영화..극장에서 개봉하는거 반갑다..</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1005</th>\n",
              "      <td>7357684</td>\n",
              "      <td>시리즈안나오나 ㅠㅠㅠㅠㅠㅠㅠㅠ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1006</th>\n",
              "      <td>9303587</td>\n",
              "      <td>끝난다는 사실이 너무 슬퍼요. 가슴이 뻥 뚫려버린것같아..</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           id                                       document  label\n",
              "1000  9856453      정말 최고의 명작 성인이 되고 본 이집트의 왕자는 또 다른 감동 그자체네요      1\n",
              "1001  6961803           이영화만 성공 했어도 스퀘어가 에닉스랑 합병 할일은 없었을텐데..      0\n",
              "1002  8681713                                   울컥하는 사회현실 ㅠㅠ      1\n",
              "1003  5348290                          기대를하나도안하면 할일없을때보기좋은영화      0\n",
              "1004  9340549  소림사 관문 통과하기 진짜 어렵다는거 보여준 영화..극장에서 개봉하는거 반갑다..      1\n",
              "1005  7357684                               시리즈안나오나 ㅠㅠㅠㅠㅠㅠㅠㅠ      1\n",
              "1006  9303587               끝난다는 사실이 너무 슬퍼요. 가슴이 뻥 뚫려버린것같아..      1"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7JNEeSSlOwq",
        "outputId": "3a0066d3-7a6c-4037-e774-8e330670b19e"
      },
      "source": [
        "# 데이터 전처리\n",
        "!pip install konlpy # konlpy 설치시 JDK 1.8, JPype1 라이브러리 버전 확인이 중요!!!"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 53.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 61.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jJMabz2lczz"
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "okt = Okt()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7cKzpsplyaj"
      },
      "source": [
        "def word_tokenization(text):\n",
        "  return [word for word in okt.morphs(text)]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_FyyVBumCIT"
      },
      "source": [
        "def preprocessing(df):\n",
        "  df = df.dropna()\n",
        "  df = df[1000:2000]\n",
        "  df['document'] = df['document'].str.replace('[^A-Za-z0-9가-힣ㄱ-ㅎㅏ-ㅣ]', '')\n",
        "  data = df['document'].apply((lambda x: word_tokenization(x)))\n",
        "  return data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Spa7d-kfmwHu",
        "outputId": "c5ba3680-ec64-4c13-bba1-41c5bb21ab35"
      },
      "source": [
        "# 텍스트 데이터1000개 전처리 후 불러오기\n",
        "review = preprocessing(df)\n",
        "len(review)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP61zYhWm5SM",
        "outputId": "39221caa-92b5-45d0-fb03-76a055f97f6e"
      },
      "source": [
        "print(review[:10])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000    [정말, 최고, 의, 명작, 성인, 이, 되, 고본, 이집트, 의, 왕자, 는, 또...\n",
            "1001    [이영화, 만, 성공했어도, 스퀘어, 가, 에, 닉스, 랑, 합병, 할, 일, 은,...\n",
            "1002                                 [울컥, 하는, 사회, 현실, ㅠㅠ]\n",
            "1003       [기대, 를, 하나, 도안, 하, 면, 할, 일, 없을, 때, 보기, 좋은, 영화]\n",
            "1004    [소림사, 관문, 통과, 하, 기, 진짜, 어렵다는거, 보여준, 영화, 극장, 에서...\n",
            "1005                              [시리즈, 안, 나오나, ㅠㅠㅠㅠㅠㅠㅠㅠ]\n",
            "1006        [끝난다는, 사실, 이, 너무, 슬퍼요, 가슴, 이, 뻥, 뚫려, 버린것, 같아]\n",
            "1007                                             [펑점, 조절]\n",
            "1008                            [와이, 건, 진짜, 으리, 으리, 한, 데]\n",
            "1009                                [손발, 이, 오, 그라드, 네, 요]\n",
            "Name: document, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwQth-KFnA2x",
        "outputId": "8e81f52f-5eed-4de7-ed86-2cd170ff02fb"
      },
      "source": [
        "# 토큰화 및 패딩\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "tokenizer = Tokenizer()\n",
        "def get_tokens(review):\n",
        "  tokenizer.fit_on_texts(review)\n",
        "  total_words = len(tokenizer.word_index)+1\n",
        "  tokenized_sentences = tokenizer.texts_to_sequences(review)\n",
        "\n",
        "  input_sequences = []\n",
        "  for token in tokenized_sentences:\n",
        "    for t in range(1, len(token)):\n",
        "      n_gram_sequence = token[:t+1]\n",
        "      input_sequences.append(n_gram_sequence)\n",
        "  return input_sequences, total_words\n",
        "input_sequences, total_words = get_tokens(review)\n",
        "input_sequences[31:40]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[730, 114, 515, 124],\n",
              " [730, 114, 515, 124, 96],\n",
              " [389, 8],\n",
              " [389, 8, 215],\n",
              " [389, 8, 215, 150],\n",
              " [389, 8, 215, 150, 17],\n",
              " [389, 8, 215, 150, 17, 61],\n",
              " [389, 8, 215, 150, 17, 61, 136],\n",
              " [389, 8, 215, 150, 17, 61, 136, 105]]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hc8qbodioWcf",
        "outputId": "51aa9560-c20b-4343-cfc1-21727d1fbac9"
      },
      "source": [
        "# 단어 사전\n",
        "print('감동 ==>> ', tokenizer.word_index['감동'])\n",
        "print('영화 ==>> ', tokenizer.word_index['영화'])\n",
        "print('코밍 ==>> ', tokenizer.word_index['코믹'])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "감동 ==>>  46\n",
            "영화 ==>>  2\n",
            "코밍 ==>>  392\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePnExBRRpaXu",
        "outputId": "0865c7fb-cf5a-4bac-e05b-7d655be97c0d"
      },
      "source": [
        "# 패딩\n",
        "max_len = max([len(word) for word in input_sequences])\n",
        "print('max_len:', max_len)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_len: 63\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCaSsIWrppiQ"
      },
      "source": [
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_len, padding='pre'))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwc05Y0gp1QH"
      },
      "source": [
        "# 입력텍스트와 타겟\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "x = input_sequences[:,:-1]\n",
        "y = to_categorical(input_sequences[:, -1], num_classes=total_words)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JzidaAPqZQ0",
        "outputId": "ce76c340-8e5e-48b2-ef2c-f82a3687fa37"
      },
      "source": [
        "# 모델\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
        "\n",
        "embedding_dim = 256\n",
        "model = Sequential([\n",
        "                    Embedding(input_dim=total_words,\n",
        "                              output_dim=embedding_dim,\n",
        "                              input_length=max_len-1),\n",
        "                    Bidirectional(LSTM(units=256)),\n",
        "                    Dense(units=total_words, activation='softmax')\n",
        "                    ])\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(x, y, epochs=20, verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "354/354 - 220s - loss: 7.8739 - accuracy: 0.0202\n",
            "Epoch 2/20\n",
            "354/354 - 215s - loss: 7.2584 - accuracy: 0.0249\n",
            "Epoch 3/20\n",
            "354/354 - 217s - loss: 6.9088 - accuracy: 0.0327\n",
            "Epoch 4/20\n",
            "354/354 - 216s - loss: 6.3978 - accuracy: 0.0464\n",
            "Epoch 5/20\n",
            "354/354 - 221s - loss: 5.6528 - accuracy: 0.0728\n",
            "Epoch 6/20\n",
            "354/354 - 217s - loss: 4.7403 - accuracy: 0.1377\n",
            "Epoch 7/20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_ZvteFlrpH2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_h7dhhxwzmx"
      },
      "source": [
        "# 문장생성함수 (시작 텍스트, 생성 단어 수)\n",
        "def text_generation(sos, count):\n",
        "  for _ in range(1, count):\n",
        "    token_list = tokenizer.texts_to_sequences([sos])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_len-1, padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list), axis=1) # 예측값의 최대 값 인텍스\n",
        "    for word, idx in tokenizer.word_index.items():\n",
        "      if idx == predicted:\n",
        "        output = word\n",
        "        break\n",
        "    sos += ' ' + output\n",
        "  return sos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34pcWfYAx7ni"
      },
      "source": [
        "text_generation('연애 하면서', 12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUc34LnKyQdb"
      },
      "source": [
        "text_generation('꿀잼', 12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0F-0FsscyTtX"
      },
      "source": [
        "text_generation('최고의 영화', 12)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}