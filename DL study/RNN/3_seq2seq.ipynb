{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "br9pK70v0GP1",
        "outputId": "9054e5c6-d267-4606-d2ea-1f709b678259"
      },
      "source": [
        "!pip install Korpora"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Korpora\n",
            "  Downloading Korpora-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▊                          | 10 kB 23.2 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 20 kB 30.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 30 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 40 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 57 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting xlrd>=1.2.0\n",
            "  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 24.2 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 20 kB 34.4 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 30 kB 27.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 40 kB 21.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 51 kB 24.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 61 kB 21.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 71 kB 23.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 81 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 92 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 96 kB 3.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.46.0 in /usr/local/lib/python3.7/dist-packages (from Korpora) (4.62.0)\n",
            "Collecting dataclasses>=0.6\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from Korpora) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from Korpora) (1.19.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->Korpora) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->Korpora) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->Korpora) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->Korpora) (2021.5.30)\n",
            "Installing collected packages: xlrd, dataclasses, Korpora\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 1.1.0\n",
            "    Uninstalling xlrd-1.1.0:\n",
            "      Successfully uninstalled xlrd-1.1.0\n",
            "Successfully installed Korpora-0.2.0 dataclasses-0.6 xlrd-2.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNQSs94N0gKe",
        "outputId": "ba08c490-9c2f-437c-aa00-3f49b3314f28"
      },
      "source": [
        "# 챗봅 라이브러리 \n",
        "from Korpora import KoreanChatbotKorpus\n",
        "corpus = KoreanChatbotKorpus()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : songys@github\n",
            "    Repository : https://github.com/songys/Chatbot_data\n",
            "    References :\n",
            "\n",
            "    Chatbot_data_for_Korean v1.0\n",
            "      1. 챗봇 트레이닝용 문답 페어 11,876개\n",
            "      2. 일상다반사 0, 이별(부정) 1, 사랑(긍정) 2로 레이블링\n",
            "    자세한 내용은 위의 repository를 참고하세요.\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[korean_chatbot_data] download ChatbotData.csv: 893kB [00:00, 9.34MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYeyZRFl1EY3",
        "outputId": "f4d0e1f1-c64d-43b6-bff4-0e125c92f03d"
      },
      "source": [
        "print(corpus.get_all_texts()[:5])\n",
        "print(corpus.get_all_pairs()[:5])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['12시 땡!', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다', '3박4일 정도 놀러가고 싶다', 'PPL 심하네']\n",
            "('하루가 또 가네요.', '위로해 드립니다.', '여행은 언제나 좋죠.', '여행은 언제나 좋죠.', '눈살이 찌푸려지죠.')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlDIrb8b1XOe",
        "outputId": "9c149b59-5ded-4387-9584-dc80122e370c"
      },
      "source": [
        "print('Q:', corpus.train[0].text)\n",
        "print('A:', corpus.train[0].pair)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q: 12시 땡!\n",
            "A: 하루가 또 가네요.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNd_HOCq1v2U",
        "outputId": "3f6428d1-02b8-4747-d068-2621512cb40b"
      },
      "source": [
        "# 데이터 전처리\n",
        "len(corpus.get_all_texts())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11823"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ijaaMVr2Ncd"
      },
      "source": [
        "# 2000개 셋만 활용\n",
        "texts = []\n",
        "pairs = []\n",
        "for i, (text, pair) in enumerate(zip(corpus.get_all_texts(), corpus.get_all_pairs())):\n",
        "  texts.append(text)\n",
        "  pairs.append(pair)\n",
        "  if i >= 2000:\n",
        "    break"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBMYXFMt2ot8",
        "outputId": "2725ba39-d3e6-4396-b16f-aeeac65c83e8"
      },
      "source": [
        "list(zip(texts, pairs))[1995:2000]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('배 아프다', '약이 필요하면 도움을 받아보세요.'),\n",
              " ('배 터지겠네', '위를 좀 쉬게 해주세요.'),\n",
              " ('배 터지겠다.', '산책 좀 해야겠네여.'),\n",
              " ('배가 너무 고파', '뭐 좀 챙겨드세요.'),\n",
              " ('배가 넘넘 고파', '저도 밥 먹고 싶어요')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oARHtwiE2yUM"
      },
      "source": [
        "import re\n",
        "def clean_sentence(sentence):\n",
        "  sentence = re.sub(r'[^0-9ㄱ-ㅎㅏ-ㅣ가-힣]', r'', sentence)\n",
        "  return sentence"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghxZq6WR3Pn1",
        "outputId": "67b044d0-de86-429f-f37f-52318f1378fe"
      },
      "source": [
        "print(clean_sentence('안녕하세요~~~~:)'))\n",
        "print(clean_sentence('텐서플로^@^#@!'))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "안녕하세요\n",
            "텐서플로\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX1B_Jda3b_y",
        "outputId": "8d482e66-c852-4bfe-e20e-838c2023f6a3"
      },
      "source": [
        "# 형태소분석기\n",
        "!pip install konlpy"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 5.0 MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 69.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcGwO3th3r_l"
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "def process_morph(sentence):\n",
        "  return ' '.join(okt.morphs(sentence))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt0q9GR536Mq"
      },
      "source": [
        "# 문장처리\n",
        "def clean_and_morph(sentence, is_question=True):\n",
        "  # 한글 문장 전처리\n",
        "  sentence = clean_sentence(sentence)\n",
        "  # 형태소\n",
        "  sentence = process_morph(sentence)\n",
        "  # Question인 경우 Answer인 경우를 분기 처리\n",
        "  if is_question:\n",
        "    return sentence\n",
        "  else:\n",
        "    # START 토큰은 decoder input에 END 토큰은 decoder output에 추가.\n",
        "    return ('<START> ' + sentence, sentence + ' <END>')\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxbTBh3H405C"
      },
      "source": [
        "def preprocess(texts, pairs):\n",
        "  questions = []\n",
        "  answer_in = []\n",
        "  answer_out = []\n",
        "  for text in texts:\n",
        "    question = clean_and_morph(text, is_question=True)\n",
        "    questions.append(question)\n",
        "  for pair in pairs:\n",
        "    in_, out_ = clean_and_morph(pair, is_question=False)\n",
        "    answer_in.append(in_)\n",
        "    answer_out.append(out_)\n",
        "\n",
        "  return questions, answer_in, answer_out"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhLO8EFF5kBL"
      },
      "source": [
        "questions, answer_in, answer_out = preprocess(texts, pairs)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwuAw3Xz50EB",
        "outputId": "ec3c68f7-5066-4ace-f685-412a6cd81245"
      },
      "source": [
        "print(questions[:2])\n",
        "print(answer_in[:2])\n",
        "print(answer_out[:2])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['12시 땡', '1 지망 학교 떨어졌어']\n",
            "['<START> 하루 가 또 가네요', '<START> 위로 해드립니다']\n",
            "['하루 가 또 가네요 <END>', '위로 해드립니다 <END>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gzoQA0O6Vzs"
      },
      "source": [
        "# 전체 문장을 하나의 리스트로\n",
        "all_sentences = questions + answer_in + answer_out"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AwGqwQD6puJ"
      },
      "source": [
        "# 토큰화\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQQZcx8g-CS2"
      },
      "source": [
        "tokenizer = Tokenizer(filters='', lower=False, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(all_sentences)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykhWcrzn-Qu8",
        "outputId": "f3ff68d6-eb37-4d05-e10a-35f61a8df8b2"
      },
      "source": [
        "# 단어사전 확인\n",
        "for word, idx in tokenizer.word_index.items():\n",
        "  print(f'{word} \\t -> \\t{idx}')\n",
        "  if idx > 10:\n",
        "    break"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<OOV> \t -> \t1\n",
            "<START> \t -> \t2\n",
            "<END> \t -> \t3\n",
            "이 \t -> \t4\n",
            "을 \t -> \t5\n",
            "가 \t -> \t6\n",
            "요 \t -> \t7\n",
            "해보세요 \t -> \t8\n",
            "는 \t -> \t9\n",
            "사람 \t -> \t10\n",
            "보세요 \t -> \t11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0WM8du1-gKd",
        "outputId": "0157cf38-d519-495d-ca5c-2ac5ca97cb8a"
      },
      "source": [
        "len(tokenizer.word_index)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4103"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0kcPPzM-mas"
      },
      "source": [
        "question_sequence = tokenizer.texts_to_sequences(questions)\n",
        "answer_in_sequence = tokenizer.texts_to_sequences(answer_in)\n",
        "answer_out_sequence = tokenizer.texts_to_sequences(answer_out)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCXf0rOA-5js"
      },
      "source": [
        "# 문장 길이 맞추기\n",
        "MAX_LENGTH = 30\n",
        "question_padded = pad_sequences(question_sequence, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
        "answer_in_padded = pad_sequences(answer_in_sequence, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
        "answer_out_padded = pad_sequences(answer_out_sequence, maxlen=MAX_LENGTH, truncating='post', padding='post')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IXGZQ-j_WRO",
        "outputId": "896908eb-0dd8-4951-c36e-a69d681accef"
      },
      "source": [
        "question_padded.shape, answer_in_padded.shape, answer_out_padded.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2001, 30), (2001, 30), (2001, 30))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l262ORpf_eHc"
      },
      "source": [
        "# 모델\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BqYwj_A_xzd"
      },
      "source": [
        "# 인코더 (학습용 Encoder)\n",
        "class Encoder(Model):\n",
        "  def __init__(self, units, vocab_size, embedding_dim, time_steps):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embedding = Embedding(vocab_size, embedding_dim, input_length=time_steps)\n",
        "    self.dropout = Dropout(0.2)\n",
        "    self.lstm = LSTM(units, return_state=True)\n",
        "  def call(self, inputs):\n",
        "    x = self.embedding(inputs)\n",
        "    x = self.dropout(x)\n",
        "    x, hidden_state, cell_state = self.lstm(x)\n",
        "    return [hidden_state, cell_state]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J12Dyjj-A941"
      },
      "source": [
        "# 디코더 (학습용 Decoder)\n",
        "class Decoder(Model):\n",
        "  def __init__(self, units, vocab_size, embedding_dim, time_steps):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.embedding = Embedding(vocab_size, embedding_dim, input_length=time_steps)\n",
        "    self.dropout = Dropout(0.2)\n",
        "    self.lstm = LSTM(units, return_state=True, return_sequences=True)\n",
        "    self.dense = Dense(vocab_size, activation='softmax')\n",
        "  def call(self, inputs, initial_state):\n",
        "    x = self.embedding(inputs)\n",
        "    x = self.dropout(x)\n",
        "    x, hidden_state, cell_state = self.lstm(x, initial_state=initial_state)\n",
        "    x = self.dense(x)\n",
        "    return x, hidden_state, cell_state"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpIetLnGBsZb"
      },
      "source": [
        "# 모델 결합\n",
        "class Seq2Seq(Model):\n",
        "  def __init__(self, units, vocab_size, embedding_dim, time_steps, start_token, end_token):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "    self.start_token = start_token\n",
        "    self.end_token = end_token\n",
        "    self.time_steps = time_steps\n",
        "    self.encoder = Encoder(units, vocab_size, embedding_dim, time_steps)\n",
        "    self.decoder = Decoder(units, vocab_size, embedding_dim, time_steps)\n",
        "  def call(self, inputs, training=True):\n",
        "    if training:\n",
        "      encoder_inputs, decoder_inputs = inputs\n",
        "      context_vector = self.encoder(encoder_inputs)\n",
        "      decoder_outputs, _, _ = self.decoder(inputs=decoder_inputs, initial_state=context_vector)\n",
        "      return decoder_outputs\n",
        "    else:\n",
        "      context_vector = self.encoder(inputs)\n",
        "      target_seq = tf.constant([[self.start_token]], dtype=tf.float32)\n",
        "      results = tf.TensorArray(tf.int32, self.time_steps)\n",
        "      for i in tf.range(self.time_steps):\n",
        "        decoder_output, decoder_hidden, decoder_cell = self.decoder(target_seq, initial_state=context_vector)\n",
        "        decoder_output = tf.cast(tf.argmax(decoder_output, axis=-1), dtype=tf.int32)\n",
        "        decoder_output = tf.reshape(decoder_output, shape=(1, 1))\n",
        "        result = results.write(i, decoder_output)\n",
        "        if decoder_output == self.end_token:\n",
        "          break\n",
        "        target_seq = decoder_output\n",
        "        context_vector = [decoder_hidden, decoder_cell]\n",
        "      return tf.reshape(results.stack(), shape=(1, self.time_steps))\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BpNkDl3EcA4"
      },
      "source": [
        "# 단어별 원핫인코딩\n",
        "VOCAB_SIZE = len(tokenizer.word_index)+1"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpb19PkHE4oi"
      },
      "source": [
        "def convert_to_one_hot(padded):\n",
        "  one_hot_vector = np.zeros((len(answer_out_padded), MAX_LENGTH, VOCAB_SIZE))\n",
        "  for i, sequence in enumerate(answer_out_padded):\n",
        "    for j, index in enumerate(sequence):\n",
        "      one_hot_vector[i, j, index] = 1\n",
        "  return one_hot_vector"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBIVdbEoFVr4",
        "outputId": "7de67aa9-22bd-466d-906c-86287e1bf7df"
      },
      "source": [
        "answer_in_one_hot = convert_to_one_hot(answer_in_padded)\n",
        "answer_out_one_hot = convert_to_one_hot(answer_out_padded)\n",
        "answer_in_one_hot[0].shape, answer_out_one_hot[0].shape"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((30, 4104), (30, 4104))"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-anA-IevFuWv"
      },
      "source": [
        "# 변환된 index를 다시 단어로 변환 함수\n",
        "def convert_index_to_text(indexs, end_token):\n",
        "  sentence = ''\n",
        "  # 모든 문장 반복\n",
        "  for index in indexs:\n",
        "    if index == end_token:\n",
        "      break\n",
        "    if index > 0 and tokenizer.index_word[index] is not None:\n",
        "      sentence += tokenizer.index_word[index]\n",
        "    else:\n",
        "      sentence += ''\n",
        "    sentence += ' '\n",
        "  return sentence"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07gvIfjOGop3"
      },
      "source": [
        "# 학습\n",
        "BUFFER_SIZER = 1000\n",
        "BATCH_SIZE = 16\n",
        "EMBEDDING_DIM = 100\n",
        "TIME_STEPS = MAX_LENGTH\n",
        "START_TOKEN = tokenizer.word_index['<START>']\n",
        "END_TOKEN = tokenizer.word_index['<END>']\n",
        "UNITS = 128\n",
        "VOCAB_SIZE = len(tokenizer.word_index)+1\n",
        "DATA_LENGTH = len(questions)\n",
        "SMAPLE_SIZE = 3\n",
        "NUM_EPOCHS = 20"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GYovKBEHSrn"
      },
      "source": [
        "checkpoint_path = 'model/seq2seq-chatbot-no-attention-checkpoint.ckpt'\n",
        "checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, save_best_only=True,\n",
        "                             monitor='loss', verbose=1)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOGrkbQwHqd1"
      },
      "source": [
        "seq2seq = Seq2Seq(UNITS, VOCAB_SIZE, EMBEDDING_DIM, TIME_STEPS, START_TOKEN, END_TOKEN)\n",
        "seq2seq.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoPTn33fIDDc"
      },
      "source": [
        "def make_prediction(model, question_inputs):\n",
        "  results = model(inputs=question_inputs, training=False)\n",
        "  results = np.asarray(results).reshape(-1)\n",
        "  return results"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUplXE_8IrD8",
        "outputId": "15455c08-f411-4dcf-ca3b-06081e93af91"
      },
      "source": [
        "for epoch in range(NUM_EPOCHS):\n",
        "  print(f'processing epoch: {epoch * 10 + 1}...')\n",
        "  seq2seq.fit([question_padded, answer_in_padded],\n",
        "              answer_out_one_hot,\n",
        "              epochs=10,\n",
        "              batch_size=BATCH_SIZE,\n",
        "              callbacks=[checkpoint])\n",
        "  # 예측 성능 검증\n",
        "  samples = np.random.randint(DATA_LENGTH, size=SMAPLE_SIZE)\n",
        "  for idx in samples:\n",
        "    question_inputs = question_padded[idx]\n",
        "    results = make_prediction(seq2seq, np.expand_dims(question_inputs, 0))\n",
        "    results = convert_index_to_text(results, END_TOKEN)\n",
        "    print(f'Q: {question[idx]}')\n",
        "    print(f'A: {results}\\n')\n",
        "    print()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing epoch: 1...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 19s 148ms/step - loss: 0.8570 - acc: 0.8615\n",
            "\n",
            "Epoch 00001: loss improved from 0.87148 to 0.85703, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 19s 147ms/step - loss: 0.8418 - acc: 0.8623\n",
            "\n",
            "Epoch 00002: loss improved from 0.85703 to 0.84181, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 19s 148ms/step - loss: 0.8279 - acc: 0.8630\n",
            "\n",
            "Epoch 00003: loss improved from 0.84181 to 0.82787, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 19s 148ms/step - loss: 0.8130 - acc: 0.8638\n",
            "\n",
            "Epoch 00004: loss improved from 0.82787 to 0.81301, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 19s 147ms/step - loss: 0.7980 - acc: 0.8645\n",
            "\n",
            "Epoch 00005: loss improved from 0.81301 to 0.79799, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 18s 143ms/step - loss: 0.7828 - acc: 0.8657\n",
            "\n",
            "Epoch 00006: loss improved from 0.79799 to 0.78278, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 18s 145ms/step - loss: 0.7674 - acc: 0.8674\n",
            "\n",
            "Epoch 00007: loss improved from 0.78278 to 0.76745, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 18s 146ms/step - loss: 0.7502 - acc: 0.8688\n",
            "\n",
            "Epoch 00008: loss improved from 0.76745 to 0.75016, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.7325 - acc: 0.8706"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZexaBSuJ2xj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}